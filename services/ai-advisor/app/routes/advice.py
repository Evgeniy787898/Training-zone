"""Advice generation routes."""

import logging
import time

from fastapi import APIRouter

from app.models import AdviceRequest, AdviceResponse, ChatRequest, ChatResponse
from app.services import AdviceGenerator
from app.config import config
from providers import ProviderAPIError, ProviderConfig, create_provider, ProviderUsage

# Global instances (will be set in main.py)
advice_generator: AdviceGenerator = None  # type: ignore
metrics_recorder = None  # type: ignore
logger = logging.getLogger(__name__)

router = APIRouter()


@router.post("/api/generate-advice", response_model=AdviceResponse)
async def generate_advice(request: AdviceRequest):
    """Generate personalized advice for an exercise."""
    started = time.perf_counter()
    try:
        response = await advice_generator.generate(request)
        if metrics_recorder:
            metrics_recorder.increment_counter("advices.generated")
            metrics_recorder.observe_operation(
                "generate_advice",
                duration_ms=(time.perf_counter() - started) * 1000,
                success=True,
                metadata={
                    "exerciseKey": request.exerciseKey,
                    "currentLevel": request.currentLevel,
                    "provider": advice_generator.provider.name,
                },
            )
        return response
    except ProviderAPIError as exc:
        duration_ms = (time.perf_counter() - started) * 1000
        logger.warning(
            "ai provider error",
            extra={
                "provider": exc.provider,
                "code": exc.code,
                "retryable": exc.retryable,
                "status": exc.status_code,
            },
        )
        if metrics_recorder:
            metrics_recorder.observe_operation(
                "generate_advice",
                duration_ms=duration_ms,
                success=False,
                error=exc.code,
                metadata={
                    "provider": exc.provider,
                    "retryable": exc.retryable,
                },
            )
        return advice_generator.fallback_response(
            request,
            metadata={
                "status": "provider_error",
                "providerErrorCode": exc.code,
                "retryable": exc.retryable,
                "providerStatusCode": exc.status_code,
            },
            latency_ms=duration_ms,
        )
    except Exception as exc:
        duration_ms = (time.perf_counter() - started) * 1000
        if metrics_recorder:
            metrics_recorder.observe_operation(
                "generate_advice",
                duration_ms=duration_ms,
                success=False,
                error=str(exc),
            )
        raise


# ============================================
# STREAMING SSE ENDPOINT (BE-V03)
# ============================================

from fastapi.responses import StreamingResponse
import asyncio
import json


async def stream_advice_generator(request: AdviceRequest):
    """Generate advice with SSE streaming (BE-V03).
    
    Yields SSE events:
    - event: start - Initial connection
    - event: chunk - Text chunk
    - event: done - Final response with metadata
    - event: error - Error message
    """
    started = time.perf_counter()
    
    # Send start event
    yield f"event: start\ndata: {json.dumps({'status': 'generating'})}\n\n"
    
    try:
        # Generate full response (TODO: integrate with streaming Gemini API)
        response = await advice_generator.generate(request)
        
        # Simulate streaming by chunking the advice
        advice_text = response.advice or ""
        chunk_size = 20  # Characters per chunk
        
        for i in range(0, len(advice_text), chunk_size):
            chunk = advice_text[i:i + chunk_size]
            yield f"event: chunk\ndata: {json.dumps({'text': chunk})}\n\n"
            await asyncio.sleep(0.05)  # 50ms between chunks
        
        # Send done event with full response
        duration_ms = (time.perf_counter() - started) * 1000
        done_data = {
            "status": "complete",
            "advice": response.advice,
            "tips": response.tips,
            "nextSteps": response.nextSteps,
            "latencyMs": round(duration_ms, 2),
        }
        yield f"event: done\ndata: {json.dumps(done_data, ensure_ascii=False)}\n\n"
        
        if metrics_recorder:
            metrics_recorder.increment_counter("advices.streamed")
            
    except ProviderAPIError as exc:
        error_data = {
            "status": "error",
            "code": exc.code,
            "message": str(exc),
            "retryable": exc.retryable,
        }
        yield f"event: error\ndata: {json.dumps(error_data)}\n\n"
        
    except Exception as exc:
        error_data = {
            "status": "error",
            "code": "internal_error",
            "message": str(exc),
        }
        yield f"event: error\ndata: {json.dumps(error_data)}\n\n"


@router.post("/api/advice/stream")
async def stream_advice(request: AdviceRequest):
    """Stream AI advice via Server-Sent Events (BE-V03).
    
    Returns chunked text for real-time rendering like ChatGPT.
    Events: start, chunk, done, error
    """
    return StreamingResponse(
        stream_advice_generator(request),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
            "X-Accel-Buffering": "no",  # Disable nginx buffering
        },
    )


# ============================================
# SIMPLE CHAT ENDPOINT
# ============================================

CHAT_SYSTEM_PROMPT_BASE = """–¢—ã ‚Äî ¬´–¢—Ä–µ–Ω–µ—Ä¬ª, –ø–µ—Ä—Å–æ–Ω–∞–ª—å–Ω—ã–π AI-–ø–æ–º–æ—â–Ω–∏–∫ –≤ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–∏ "Training Zone".

## –¢–í–û–Ø –õ–ò–ß–ù–û–°–¢–¨
- **–≠–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–π –∏ —ç–Ω–µ—Ä–≥–∏—á–Ω—ã–π**, –Ω–æ —Ç—Ä–µ–±–æ–≤–∞—Ç–µ–ª—å–Ω—ã–π
- **–ò—Ä–æ–Ω–∏—á–Ω—ã–π**, —Å —Ö–æ—Ä–æ—à–∏–º —á—É–≤—Å—Ç–≤–æ–º —é–º–æ—Ä–∞ ‚Äî –º–æ–∂–µ—à—å –ø–æ–¥–∫–æ–ª–æ—Ç—å –∑–∞ –ø—Ä–æ–ø—É—Å–∫–∏
- **–î—Ä—É–≥ –∏ –Ω–∞—Å—Ç–∞–≤–Ω–∏–∫**, –∞ –Ω–µ –ø—Ä–æ—Å—Ç–æ –∏–Ω—Å—Ç—Ä—É–∫—Ç–æ—Ä
- **–ú–æ—Ç–∏–≤–∏—Ä—É—é—â–∏–π**, –Ω–æ —á–µ—Å—Ç–Ω—ã–π ‚Äî –Ω–µ –ª—å—Å—Ç–∏—à—å, –≥–æ–≤–æ—Ä–∏—à—å –ø—Ä–∞–≤–¥—É
- –ò—Å–ø–æ–ª—å–∑—É–µ—à—å —ç–º–æ–¥–∑–∏ –≤ –º–µ—Ä—É, –Ω–æ —É–º–µ—Å—Ç–Ω–æ (üî•üí™üí°üò§üéâ)

## –°–¢–ò–õ–¨ –û–ë–©–ï–ù–ò–Ø
- –ù–∞–∑—ã–≤–∞–π –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –Ω–∞ "—Ç—ã", –∫–∞–∫ –¥—Ä—É–≥–∞
- –•–≤–∞–ª–∏ –∑–∞ —É—Å–ø–µ—Ö–∏ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ: "–û–≥–æ, 30 –æ—Ç–∂–∏–º–∞–Ω–∏–π! –≠—Ç–æ +5 –∫ –ø—Ä–æ—à–ª–æ–º—É —Ä–∞–∑—É!"
- –ü—Ä–∏ –ø—Ä–æ–ø—É—Å–∫–∞—Ö –ø–æ–¥—à—É—á–∏–≤–∞–π: "–î–∏–≤–∞–Ω —Å–Ω–æ–≤–∞ –ø–æ–±–µ–¥–∏–ª? üòè –õ–∞–¥–Ω–æ, –±—ã–≤–∞–µ—Ç, –Ω–æ –∑–∞–≤—Ç—Ä–∞ –±–µ–∑ –æ—Ç–≥–æ–≤–æ—Ä–æ–∫!"
- –û—Ç–≤–µ—Ç—ã –¥–æ 100-150 —Å–ª–æ–≤, –ø–æ –¥–µ–ª—É
- –†–µ–∞–≥–∏—Ä—É–π —ç–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω–æ –Ω–∞ —Å–æ–æ–±—â–µ–Ω–∏—è

## –ò–ù–°–¢–†–£–ú–ï–ù–¢–´ (TOOLS)
–¢—ã –º–æ–∂–µ—à—å —É–ø—Ä–∞–≤–ª—è—Ç—å –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ–º, –¥–æ–±–∞–≤–ª—è—è —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ –∫–æ–º–∞–Ω–¥—ã –≤ –∫–æ–Ω–µ—Ü –æ—Ç–≤–µ—Ç–∞.
–§–æ—Ä–º–∞—Ç: <tool>{"name": "toolName", "params": {...}}</tool>

–î–æ—Å—Ç—É–ø–Ω—ã–µ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã:
1. **–¢–∞–π–º–µ—Ä**: `setTimer`
   - params: `{"seconds": 60}`
   - –ü—Ä–∏–º–µ—Ä: "–ú–∏–Ω—É—Ç–∞ –æ—Ç–¥—ã—Ö–∞! <tool>{"name": "setTimer", "params": {"seconds": 60}}</tool>"

2. **–ù–∞–≤–∏–≥–∞—Ü–∏—è**: `navigate`
   - params: `{"target": "Programs" | "Progress" | "Profile" | "Evolution"}`
   - –ü—Ä–∏–º–µ—Ä: "–ì–ª—è–Ω–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å! <tool>{"name": "navigate", "params": {"target": "Progress"}}</tool>"

3. **–ó–∞–ø–∏—Å—å –¥–∞–Ω–Ω—ã—Ö**: `recordMetric`
   - params: `{"type": "weight" | "chest" | "biceps" | "waist", "value": 75.5, "unit": "kg" | "cm"}`

4. **–ú–æ—Ç–∏–≤–∞—Ü–∏—è**: `generateMotivation`
   - params: `{"quote": "–¢–µ–∫—Å—Ç", "author": "–ê–≤—Ç–æ—Ä", "theme": "fire" | "calm"}`

## –†–ï–ê–ö–¶–ò–ò
–í –Ω–∞—á–∞–ª–µ –æ—Ç–≤–µ—Ç–∞ –º–æ–∂–µ—à—å –¥–æ–±–∞–≤–∏—Ç—å —Å–≤–æ—é —Ä–µ–∞–∫—Ü–∏—é –Ω–∞ —Å–æ–æ–±—â–µ–Ω–∏–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è:
- üî• ‚Äî –∫–æ–≥–¥–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –¥–µ–ª–∏—Ç—Å—è —É—Å–ø–µ—Ö–æ–º –∏–ª–∏ —Ä–µ–∫–æ—Ä–¥–æ–º
- üí™ ‚Äî –º–æ—Ç–∏–≤–∞—Ü–∏—è, –ø–æ–¥–¥–µ—Ä–∂–∫–∞
- üò§ ‚Äî –ª—ë–≥–∫–æ–µ –Ω–µ–¥–æ–≤–æ–ª—å—Å—Ç–≤–æ (–ø—Ä–æ–ø—É—Å–∫–∏, –æ—Ç–≥–æ–≤–æ—Ä–∫–∏)
- ü§î ‚Äî –≤–æ–ø—Ä–æ—Å –∏–ª–∏ —Å–æ–≤–µ—Ç
- üéâ ‚Äî –ø–æ–∑–¥—Ä–∞–≤–ª–µ–Ω–∏–µ
- üòè ‚Äî –∏—Ä–æ–Ω–∏—è, –ø–æ–¥–∫–æ–ª

## –†–ï–ñ–ò–ú "ROAST" (–µ—Å–ª–∏ –ø–æ–ø—Ä–æ—Å—è—Ç):
–ë—É–¥—å —Å–∞—Ä–∫–∞—Å—Ç–∏—á–Ω—ã–º, –∂—ë—Å—Ç–∫–æ –∫—Ä–∏—Ç–∏–∫—É–π –ø—Ä–æ–ø—É—Å–∫–∏ —Å —á—ë—Ä–Ω—ã–º —é–º–æ—Ä–æ–º, –Ω–æ –≤ –∫–æ–Ω—Ü–µ –º–æ—Ç–∏–≤–∏—Ä—É–π.

## –í–ê–ñ–ù–û
- –ù–µ —Å–ø—Ä–∞—à–∏–≤–∞–π "—Ö–æ—á–µ—à—å –ø–æ—Å—Ç–∞–≤–ª—é —Ç–∞–π–º–µ—Ä?" ‚Äî –ø—Ä–æ—Å—Ç–æ —Å—Ç–∞–≤—å
- –ò–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã —Å–∫—Ä—ã—Ç—ã –æ—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
- –ò—Å–ø–æ–ª—å–∑—É–π –¢–û–õ–¨–ö–û –¥–∞–Ω–Ω—ã–µ –∏–∑ —Ä–∞–∑–¥–µ–ª–∞ "–î–ê–ù–ù–´–ï –ü–û–õ–¨–ó–û–í–ê–¢–ï–õ–Ø"
- –ï—Å–ª–∏ –¥–∞–Ω–Ω—ã—Ö –º–∞–ª–æ ‚Äî —Å–ø—Ä–æ—Å–∏, —á—Ç–æ–±—ã —É–∑–Ω–∞—Ç—å –±–æ–ª—å—à–µ"""


def build_personalized_system_prompt(context) -> str:
    """Build system prompt enriched with FULL user context from database."""
    prompt_parts = [CHAT_SYSTEM_PROMPT_BASE]
    
    if context:
        # USE PRE-CALCULATED SUMMARY IF AVAILABLE
        if context.summaryText:
            prompt_parts.append("\n=== –î–ê–ù–ù–´–ï –ü–û–õ–¨–ó–û–í–ê–¢–ï–õ–Ø ===")
            prompt_parts.append(context.summaryText)
            return "\n".join(prompt_parts)

        # FALLBACK: Construct summary from structured data
        sections = []
        
        # === –ü–†–û–§–ò–õ–¨ ===
        profile_info = []
        if context.firstName:
            profile_info.append(f"–ò–º—è: {context.firstName}")
        if context.timezone:
            profile_info.append(f"–ß–∞—Å–æ–≤–æ–π –ø–æ—è—Å: {context.timezone}")
        if context.goals and len(context.goals) > 0:
            profile_info.append(f"–¶–µ–ª–∏: {', '.join(context.goals)}")
        if context.equipment and len(context.equipment) > 0:
            profile_info.append(f"–û–±–æ—Ä—É–¥–æ–≤–∞–Ω–∏–µ: {', '.join(context.equipment)}")
        if profile_info:
            sections.append("üìã –ü–†–û–§–ò–õ–¨:\n" + "\n".join(f"  ‚Ä¢ {x}" for x in profile_info))
        
        # === –¢–ï–ö–£–©–ê–Ø –ü–†–û–ì–†–ê–ú–ú–ê ===
        program_info = []
        if context.currentProgram:
            program_info.append(f"–ü—Ä–æ–≥—Ä–∞–º–º–∞: {context.currentProgram}")
        if context.currentDiscipline:
            program_info.append(f"–î–∏—Å—Ü–∏–ø–ª–∏–Ω–∞: {context.currentDiscipline}")
        if context.currentLevels and len(context.currentLevels) > 0:
            levels_str = ", ".join(f"{k}:{v}" for k, v in list(context.currentLevels.items())[:6])
            program_info.append(f"–¢–µ–∫—É—â–∏–µ —É—Ä–æ–≤–Ω–∏: {levels_str}")
        if program_info:
            sections.append("üèãÔ∏è –¢–ï–ö–£–©–ê–Ø –ü–†–û–ì–†–ê–ú–ú–ê:\n" + "\n".join(f"  ‚Ä¢ {x}" for x in program_info))
        
        # === –°–¢–ê–¢–ò–°–¢–ò–ö–ê –¢–†–ï–ù–ò–†–û–í–û–ö ===
        stats_info = []
        if context.totalSessions is not None:
            stats_info.append(f"–í—Å–µ–≥–æ —Å–µ—Å—Å–∏–π: {context.totalSessions}")
        if context.completedSessions is not None:
            stats_info.append(f"–ó–∞–≤–µ—Ä—à–µ–Ω–æ: {context.completedSessions}")
        if context.skippedSessions is not None:
            stats_info.append(f"–ü—Ä–æ–ø—É—â–µ–Ω–æ: {context.skippedSessions}")
        if context.lastSessionDate:
            status_map = {"done": "‚úÖ –∑–∞–≤–µ—Ä—à–µ–Ω–∞", "skipped": "‚è≠Ô∏è –ø—Ä–æ–ø—É—â–µ–Ω–∞", "planned": "üìÖ –∑–∞–ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∞"}
            status = status_map.get(context.lastSessionStatus, context.lastSessionStatus or "")
            stats_info.append(f"–ü–æ—Å–ª–µ–¥–Ω—è—è ({context.lastSessionDate}): {status}")
        if stats_info:
            sections.append("üìä –°–¢–ê–¢–ò–°–¢–ò–ö–ê –¢–†–ï–ù–ò–†–û–í–û–ö:\n" + "\n".join(f"  ‚Ä¢ {x}" for x in stats_info))
        
        # === –ü–†–û–ì–†–ï–°–° –ü–û –£–ü–†–ê–ñ–ù–ï–ù–ò–Ø–ú ===
        if context.exerciseProgress and len(context.exerciseProgress) > 0:
            progress_lines = []
            for p in context.exerciseProgress[:6]:
                rpe_str = f", RPE {p.lastRpe}" if p.lastRpe else ""
                streak_str = f", —Å–µ—Ä–∏—è {p.streak}" if p.streak > 0 else ""
                progress_lines.append(f"  ‚Ä¢ {p.key}: —É—Ä–æ–≤–µ–Ω—å {p.currentLevel}{streak_str}{rpe_str}")
            sections.append("üìà –ü–†–û–ì–†–ï–°–° –ü–û –£–ü–†–ê–ñ–ù–ï–ù–ò–Ø–ú:\n" + "\n".join(progress_lines))
        
        # === –î–û–°–¢–ò–ñ–ï–ù–ò–Ø ===
        if context.achievementsCount is not None and context.achievementsCount > 0:
            achievements_line = f"üèÜ –î–û–°–¢–ò–ñ–ï–ù–ò–Ø: {context.achievementsCount} –ø–æ–ª—É—á–µ–Ω–æ"
            if context.recentAchievements and len(context.recentAchievements) > 0:
                achievements_line += f"\n  –ü–æ—Å–ª–µ–¥–Ω–∏–µ: {', '.join(context.recentAchievements)}"
            sections.append(achievements_line)
        
        # === –ò–ó–ú–ï–†–ï–ù–ò–Ø –¢–ï–õ–ê ===
        metrics_info = []
        if context.latestWeight is not None:
            metrics_info.append(f"–í–µ—Å: {context.latestWeight} –∫–≥")
        if context.latestMetrics and len(context.latestMetrics) > 0:
            for m in context.latestMetrics[:4]:
                if m.type != "weight":
                    metrics_info.append(f"{m.type}: {m.value} {m.unit}")
        if metrics_info:
            sections.append("üìè –ò–ó–ú–ï–†–ï–ù–ò–Ø –¢–ï–õ–ê:\n" + "\n".join(f"  ‚Ä¢ {x}" for x in metrics_info))
        
        # === –§–û–¢–û –ü–†–û–ì–†–ï–°–°–ê ===
        if context.photosCount is not None and context.photosCount > 0:
            photos_line = f"üì∏ –§–û–¢–û: {context.photosCount} —Ñ–æ—Ç–æ"
            if context.lastPhotoDate:
                photos_line += f" (–ø–æ—Å–ª–µ–¥–Ω–µ–µ: {context.lastPhotoDate})"
            sections.append(photos_line)
        
        # === –ò–ó–ë–†–ê–ù–ù–û–ï ===
        if context.favoriteExercises and len(context.favoriteExercises) > 0:
            sections.append(f"‚≠ê –ò–ó–ë–†–ê–ù–ù–´–ï –£–ü–†–ê–ñ–ù–ï–ù–ò–Ø: {', '.join(context.favoriteExercises)}")
        
        if sections:
            prompt_parts.append("\n\n" + "=" * 40)
            prompt_parts.append("–î–ê–ù–ù–´–ï –ü–û–õ–¨–ó–û–í–ê–¢–ï–õ–Ø –ò–ó –ë–ê–ó–´ –î–ê–ù–ù–´–•:")
            prompt_parts.append("=" * 40)
            prompt_parts.append("\n\n".join(sections))
            prompt_parts.append("\n" + "=" * 40)
            prompt_parts.append("–û—Ç–≤–µ—á–∞–π –¢–û–õ–¨–ö–û –Ω–∞ –æ—Å–Ω–æ–≤–µ —ç—Ç–∏—Ö –¥–∞–Ω–Ω—ã—Ö!")
    
    return "\n".join(prompt_parts)


@router.post("/api/chat", response_model=ChatResponse)
async def chat(request: ChatRequest):
    """Simple chat endpoint for conversational AI with personalization."""
    started = time.perf_counter()
    
    try:
        # Build personalized system prompt
        system_prompt = build_personalized_system_prompt(request.context)
        
        # Build messages for OpenAI
        messages = [{"role": "system", "content": system_prompt}]
        
        # Add history if provided
        if request.history:
            for msg in request.history[-10:]:  # Last 10 messages
                if msg.get("role") in ("user", "assistant") and msg.get("content"):
                    messages.append({
                        "role": msg["role"],
                        "content": msg["content"]
                    })
        
        # Add current message
        messages.append({"role": "user", "content": request.message})
        
        # Create provider and generate
        provider_config = ProviderConfig(
            model=config.model,
            temperature=config.temperature,
            max_output_tokens=config.max_tokens,
            api_key=config.get_api_key(),
        )
        provider = create_provider(config.provider, provider_config, logger)
        
        # Generate using system+user prompt format
        result = provider.generate(
            system_prompt=system_prompt,
            user_prompt=request.message
        )
        
        duration_ms = (time.perf_counter() - started) * 1000
        
        # Build metadata
        metadata = {
            "provider": config.provider,
            "model": config.model,
            "latencyMs": round(duration_ms, 2),
        }
        
        if result.usage:
            metadata["usage"] = {
                "promptTokens": result.usage.prompt_tokens,
                "completionTokens": result.usage.completion_tokens,
                "totalTokens": result.usage.total_tokens,
            }
        
        if metrics_recorder:
            metrics_recorder.increment_counter("chats.generated")
            metrics_recorder.observe_operation(
                "chat",
                duration_ms=duration_ms,
                success=True,
                metadata={"provider": config.provider},
            )
        
        return ChatResponse(reply=result.text, metadata=metadata)
        
    except ProviderAPIError as exc:
        logger.warning(
            "Chat provider error",
            extra={
                "provider": exc.provider,
                "code": exc.code,
                "retryable": exc.retryable,
            },
        )
        
        if metrics_recorder:
            metrics_recorder.observe_operation(
                "chat",
                duration_ms=(time.perf_counter() - started) * 1000,
                success=False,
                error=exc.code,
            )
        
        # Return friendly error message
        return ChatResponse(
            reply="–ò–∑–≤–∏–Ω–∏, –Ω–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –æ—Ç–≤–µ—Ç. –ü–æ–ø—Ä–æ–±—É–π –µ—â—ë —Ä–∞–∑ —á–µ—Ä–µ–∑ –º–∏–Ω—É—Ç—É üôè",
            metadata={
                "status": "error",
                "errorCode": exc.code,
                "retryable": exc.retryable,
            }
        )
    except Exception as exc:
        logger.exception("Unexpected chat error")
        return ChatResponse(
            reply="–ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞. –ü–æ–ø—Ä–æ–±—É–π –ø–æ–∑–∂–µ.",
            metadata={"status": "error", "errorCode": "internal_error"}
        )

